{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EC 01 Volcanic activity detection and noise characterization using machine learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Linear regression prediction of height point."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Author(s)\n",
    "\n",
    "\n",
    "- Author1 = {\"name\": \"Myles Mason\", \"affiliation\": \"Virginia Tech\", \"email\": \"mylesm18@vt.edu\", \"orcid\": \"0000-0002-8811-8294\"}\n",
    "- Author2 = {\"name\": \"John Wenskovitch\", \"affiliation\": \"Virginia Tech\", \"email\": \"jw87@vt.edu\", \"orcid\": \"0000-0002-0573-6442\"}\n",
    "- Author3 = {\"name\": \"D. Sarah Stamps\", \"affiliation\": \"Virginia Tech\", \"email\": \"dstamps@vt.edu\",\"orcid\": \"0000-0002-3531-1752\"}\n",
    "- Author4 = {\"name\": \"Joshua Robert Jones\", \"affiliation\": \"Virginia Tech\", \"email\": \"joshj55@vt.edu\", \"orcid\": \"0000-0002-6078-4287\"}\n",
    "- Author5 = {\"name\": \"Mike Dye\", \"affiliation\": \"Unaffiliated\", \"email\": \"mike@mikedye.com\", \"orcid\": \" 0000-0003-2065-870X\"}\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Purpose\n",
    "\n",
    "This Jupyter notebook explores methods towards characterizing noise and eventually predicting volcanic activity for [Ol Doinyo Lengai](http://tzvolcano.chordsrt.com)\n",
    " (an active volcano in Tanzania) with machine learning. Machine learning is a powerful tool that enables the automatization of complex mathematical and analytical models. In this Jupyter notebook, the components are time, height, latitude, and longitude. The predicted component values are the following heights. This project uses Global Navigation Satellite System (GNSS) data from the EarthCube CHORDS portal TZVOLCANO (Stamps et al. 2016; Daniels et al., 2016; Kerkez et al., 2016), which is the online interface for obtaining open-access real-time positioning data collected around Ol Doinyo Lengai(http://tzvolcano.chordsrt.com). The bulk of the project is the exploration of the data and later prediction of height points. The station that this project analyzes is OLO1 for days 12/16/2020 and 04/16/2021.  \n",
    "\n",
    "## Technical contributions\n",
    "- The training of the models and analysis uses basic linear algebra and statistics \n",
    "- The main libraries used (NumPy and pandas) are both libraries for data manipulation and linear algebra \n",
    "- The CHORDS site linked above is the location of the data and the interface of [CHORDS](http://tzvolcano.chordsrt.com)\n",
    "- Implementation of Linear Regression for prediction on time-series data\n",
    "\n",
    "## Methodology\n",
    "- Data collection\n",
    "- Data pre-processing\n",
    "- Input data for the linear regression model\n",
    "- Increase sample size of inputs for the model\n",
    "- Compare predictions to actual data\n",
    "\n",
    "## Results\n",
    "This notebook explored predicting height data from the TZVOLCANO CHORDS portal using Linear Regression from different days. It also evaluates how much test data is needed to best predict height data. We find that having 10% test data yields the best results for predictions with the Mean Squared Error of 8.324537406984948e-05. For predictions from data inputted and predicted from a single day we find the 75% test data yields the best results with an average error of -0.00010741432729476694.\n",
    "\n",
    "## Funding\n",
    "\n",
    "- Award1 = {\"agency\": \"National Science Foundation EarthCube Program\", \"award_code\": \"1639554\", \"award_URL\": https://www.nsf.gov/awardsearch/showAward?AWD_ID=1639554&HistoricalAwards=false }\n",
    "- Award2 = {\"agency\": \"Virginia Tech Academy of Integrated Sciences Hamlett Undergraduate Research Award\", \"award_code\": \"44672\", \"award_URL\": \"\"}\n",
    "\n",
    "## Keywords\n",
    "\n",
    "\n",
    "keywords=[\"tzDF\", \"Linear Regression\", \"Concat\", \"Transpose\",\"Mean Squared Error(MSE)\"]\n",
    "\n",
    "## Citation\n",
    "Mason, Myles, John Wenskovitch, D. Sarah Stamps, Joshua Robert Jones,  Mike Dye (2021), EC_01_Volcanic_activity_detection_and_noise_characterization_using_machine learning, EarthCube Annual Meeting.\n",
    "\n",
    "## Suggested next steps\n",
    "The next step for this notebook will be increasing the inputs for the prediction model. More analysis of the noise will be crucial in the next step for further volcanic activity prediction. They are explicitly generating synthetic data that will mimic volcanic activity.\n",
    "## Acknowledgements \n",
    "- Virginia Tech Department of Geosciences \n",
    "- Alice and Luther Hamlet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup\n",
    "\n",
    "## Library import\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data manipulation\n",
    "import pandas as pd\n",
    "import json\n",
    "import numpy as np\n",
    "from datetime import datetime as dt\n",
    "\n",
    "# Visualizations\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Modeling\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parameter definitions\n",
    "- 'tzDF': intial dataframe contains 12/06/2020 data \n",
    "- 'tz2DF': secondary dataframe contains 04/16/2021 data\n",
    "- 'ONE_THROUGH_TWENTY': array of values in tzDF[\"measurements_height\"] used for prediction\n",
    "- 'TWO_THROUGH_TWENTY_ONE':array of values in tzDF[\"measurements_height\"] used for prediction\n",
    "- 'THREE_THROUGH_TWENTY_TWO': array of values in tzDF[\"measurements_height\"] used for prediction\n",
    "- 'FOUR_THROUGH_TWENTY_THREE': array of values in tzDF[\"measurements_height\"] used for prediction\n",
    "- 'DECEMBER_SERIES_X': inputted height for 12/16/2020 data for Linear Regression \n",
    "- 'DECEMBER_SERIES_Y': target height data for 12/16/2020\n",
    "- 'APRIL_SERIES_X': inputted height data for 04/16/2021\n",
    "- 'APRIL_SERIES_Y': target height data for 04/16/2021\n",
    "- 'APRIL_PREDICTION': Predicted values from the two days\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Data import\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The CHORDS notebook portal is where the data is acessed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import for JSON files for manipulation\n",
    "''' Both files are station one, but the first date is December 16,2020\n",
    "    while the second date is April 16 2021.\n",
    "'''\n",
    "with open('OLO1_12_16_20.geojson', 'r', encoding=\"utf-8\") as infile:\n",
    "    tzList = json.load(infile)\n",
    "\n",
    "with open('OLO1_4_16_21.geojson', 'r', encoding=\"utf-8\") as infile:\n",
    "    tz2List = json.load(infile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data processing and analysis\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Convert both JSON's into  a partially-flattened pandas DataFrame   \n",
    "tzDF =  pd.json_normalize(tzList[\"features\"][0][\"properties\"][\"data\"], sep='_')\n",
    "tz2DF =  pd.json_normalize(tzList[\"features\"][0][\"properties\"][\"data\"], sep='_')\n",
    "#Overview of numerical elements of the data\n",
    "print(tzDF.describe())\n",
    "tzDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the .describe call looking at our data frame, there does not seem to be much variation in columns measurements_lat and measurements_lon. The reason for choosing to explore the measurements_height column in this notebook is because of its interpretation. From viewing the data frame, the time column is in a time series form with extra separator variables \"T\" and \"Z\" in the following two cells; we will convert the timestamp column into an integer form for easy manipulation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Function to convert timestamp column of tzDF and tz2DF from time series to an integer for easy manipulation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def timeconvertfunc(timestamp):\n",
    " #The format of tzDF[\"measurments_height] is in string form and timeseries so this method make it an integer\"\n",
    "  ts = pd.Timestamp(timestamp, tz=None).to_pydatetime()\n",
    "  ts = 3600*ts.hour + 60*ts.minute + ts.second\n",
    "\n",
    "  return ts\n",
    "#Applying above method to the two data frames\n",
    "tzDF[\"timeconvert\"] = tzDF[\"time\"].apply(timeconvertfunc)\n",
    "tz2DF[\"timeconvert\"] = tzDF[\"time\"].apply(timeconvertfunc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualization of basic statistics from measurements_height and linear regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the code block below the four series objects are partitions of the measurements_height column in the tzDF DataFrame. We create these partitions to feed into a linear regression model for predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#From the first\n",
    "\n",
    "ONE_THROUGH_TWENTY = tzDF[\"measurements_height\"].loc[1:21].values.reshape(-1,1)\n",
    "TWO_THROUGH_TWENTY_ONE = tzDF[\"measurements_height\"].loc[2:22].values.reshape(-1,1)\n",
    "THREE_THROUGH_TWENTY_TWO =tzDF[\"measurements_height\"].loc[3:23].values.reshape(-1,1)\n",
    "FOUR_THROUGH_TWENTY_FOUR = tzDF[\"measurements_height\"].loc[4:24].values.reshape(-1,1)\n",
    "\n",
    "#Linear Regression model on columns 1-20 and 2-21\n",
    "\n",
    "lm = LinearRegression()\n",
    "lm.fit(ONE_THROUGH_TWENTY ,TWO_THROUGH_TWENTY_ONE)\n",
    "y_pred = lm.predict(ONE_THROUGH_TWENTY)\n",
    "plt.xlabel(\"actual height(meters)\")\n",
    "plt.ylabel(\"predicted height(meters)\")\n",
    "plt.title(\"actual height vs predicted height\")\n",
    "plt.scatter(ONE_THROUGH_TWENTY,TWO_THROUGH_TWENTY_ONE)\n",
    "plt.plot(ONE_THROUGH_TWENTY,y_pred,color=\"red\")\n",
    "plt.show()\n",
    "\n",
    "print(\"The b value is\",np.round(lm.coef_[0][0],4))\n",
    "\n",
    "print(\"The m value is\",np.round(lm.intercept_[0],4))\n",
    "\n",
    "\n",
    "print(\"The R^2 value is\",np.round(lm.score(ONE_THROUGH_TWENTY,TWO_THROUGH_TWENTY_ONE),4),)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the above model using the first through the twentieth column and second, through the twenty-second column, we yield a Coefficient of Correlation (R^2) value of about 0.73, which shows a positive correlation between the two inputs. So about 37% of the variation is residing in the residual."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TWO_THROUGH_TWENTY_ONE and THREE_THROUGH_TWENTY_TWO "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "lm = LinearRegression()\n",
    "lm.fit(TWO_THROUGH_TWENTY_ONE ,THREE_THROUGH_TWENTY_TWO)\n",
    "y_pred1 = lm.predict(TWO_THROUGH_TWENTY_ONE)\n",
    "plt.scatter(TWO_THROUGH_TWENTY_ONE,THREE_THROUGH_TWENTY_TWO)\n",
    "plt.plot(TWO_THROUGH_TWENTY_ONE,y_pred1,color=\"green\")\n",
    "plt.xlabel(\"actual height(meters)\")\n",
    "plt.ylabel(\"predicted height(meters)\")\n",
    "plt.title(\"actual height vs predicted height\")\n",
    "plt.show()\n",
    "\n",
    "\n",
    "print(\"This is the b value\",np.round(lm.coef_[0][0],4))\n",
    "print(\"This is the m value\",np.round(lm.intercept_[0],4))\n",
    "print(\"This is the R^2\",np.round(lm.score(TWO_THROUGH_TWENTY_ONE,THREE_THROUGH_TWENTY_TWO),4))\n",
    "\n",
    "def to_precision(x,p):\n",
    "    \"\"\"\n",
    "    returns a string representation of x formatted with a precision of p\n",
    "\n",
    "    Based on the webkit javascript implementation taken from here:\n",
    "    https://code.google.com/p/webkit-mirror/source/browse/JavaScriptCore/kjs/number_object.cpp\n",
    "    \"\"\"\n",
    "\n",
    "    x = float(x)\n",
    "\n",
    "    if x == 0.:\n",
    "        return \"0.\" + \"0\"*(p-1)\n",
    "\n",
    "    out = []\n",
    "\n",
    "    if x < 0:\n",
    "        out.append(\"-\")\n",
    "        x = -x\n",
    "\n",
    "    e = int(math.log10(x))\n",
    "    tens = math.pow(10, e - p + 1)\n",
    "    n = math.floor(x/tens)\n",
    "\n",
    "    if n < math.pow(10, p - 1):\n",
    "        e = e -1\n",
    "        tens = math.pow(10, e - p+1)\n",
    "        n = math.floor(x / tens)\n",
    "\n",
    "    if abs((n + 1.) * tens - x) <= abs(n * tens -x):\n",
    "        n = n + 1\n",
    "\n",
    "    if n >= math.pow(10,p):\n",
    "        n = n / 10.\n",
    "        e = e + 1\n",
    "\n",
    "    m = \"%.*g\" % (p, n)\n",
    "\n",
    "    if e < -2 or e >= p:\n",
    "        out.append(m[0])\n",
    "        if p > 1:\n",
    "            out.append(\".\")\n",
    "            out.extend(m[1:p])\n",
    "        out.append('e')\n",
    "        if e > 0:\n",
    "            out.append(\"+\")\n",
    "        out.append(str(e))\n",
    "    elif e == (p -1):\n",
    "        out.append(m)\n",
    "    elif e >= 0:\n",
    "        out.append(m[:e+1])\n",
    "        if e+1 < len(m):\n",
    "            out.append(\".\")\n",
    "            out.extend(m[e+1:])\n",
    "    else:\n",
    "        out.append(\"0.\")\n",
    "        out.extend([\"0\"]*-(e+1))\n",
    "        out.append(m)\n",
    "\n",
    "    return \"\".join(out)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the above model using the first through the twentieth column and the second through the twenty-second column, we yield a Coefficient of Correlation (R^2) value of about 0.72, which shows a positive correlation between the two inputs. So about 38% of the variation is residing in the residual."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Regression from single day"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code chunk uses  Linear Regression (specifically with rows of height measurement) for  one_Through_Twenty,two_Through_Twenty_One, and \n",
    "three_Through_Twenty_Two. The data frame used for the model is tzDF, and we display the predicted values versus actual values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Set up series object for partions in the dataframe\n",
    "ONE_THROUGH_TWENTY = tzDF[\"measurements_height\"].loc[1:21]\n",
    "TWO_THROUGH_TWENTY_ONE= tzDF[\"measurements_height\"].loc[2:22]\n",
    "THREE_THROUGH_TWENTY_TWO =tzDF[\"measurements_height\"].loc[3:23]\n",
    "FOUR_THROUGH_TWENTY_THREE = tzDF[\"measurements_height\"].loc[4:24]\n",
    "\n",
    "#Renaming the series objects\n",
    "ONE_THROUGH_TWENTY.rename({\"measurements_height\":\"w\"},axis =1,inplace=True)\n",
    "TWO_THROUGH_TWENTY_ONE.rename({\"measurements_height\":\"x\"},axis =1,inplace=True)\n",
    "THREE_THROUGH_TWENTY_TWO.rename({\"measurements_height\":\"y\"},axis =1,inplace=True)\n",
    "FOUR_THROUGH_TWENTY_THREE.rename({\"measurements_height\":\"z\"},axis =1,inplace=True)\n",
    "\n",
    "#Concating the series objects to one dataframe, result_DF\n",
    "result_DF = pd.concat([ONE_THROUGH_TWENTY,TWO_THROUGH_TWENTY_ONE,THREE_THROUGH_TWENTY_TWO,FOUR_THROUGH_TWENTY_THREE],axis=1)\n",
    "result_DF\n",
    "\n",
    "#Modefying dataframe by shifting the coulmns up \n",
    "result_DF.iloc[:,1] = result_DF.iloc[:,1].shift(-1)\n",
    "result_DF.iloc[:,2] = result_DF.iloc[:,2].shift(-2)\n",
    "result_DF.iloc[:,3] = result_DF.iloc[:,3].shift(-3)\n",
    "\n",
    "#Aligning all coulmns of the data frame together\n",
    "result_DF = result_DF.dropna()\n",
    "result_DF = result_DF.transpose()\n",
    "result_DF\n",
    "\n",
    "#Linear Regression on all rows and 1-19 coulmns \n",
    "lm = LinearRegression()\n",
    "x = result_DF.iloc[:,0:20]\n",
    "y = result_DF.iloc[:,20]\n",
    "lm.fit(x,y)\n",
    "y_pred1 = lm.predict(x)\n",
    "plt.scatter(y,y_pred1)\n",
    "plt.xlabel(\"actual height(m)\")\n",
    "plt.ylabel(\"predicted height(m)\")\n",
    "plt.title(\"actual height vs predicted height\")\n",
    "plt.show()\n",
    "#print(\"This is the b value\",lm.coef_,)\n",
    "#print(\"This is the m value\",lm.intercept_)\n",
    "#print(\"This is the R^2\",lm.score(x,y))\n",
    "print(y-y_pred1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the above graph, all points are perfectly predicted as the values overlap graphically, and the height difference is zero. From viewing our results, we are gaining perfect accuracy of prediction we will now increase the sample size of the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Method to increase sample size for the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The method below takes in a data frame, goes into measurement height, and gets the first through nineteenth values in the height data frame. The first and nineteenth values are increased by one thousand times. The Data frame is transposed into columns then added to an empty list to be concatenated into a data frame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Making an empty list to store coulmn values\n",
    "empty_list = []\n",
    "def make_list(DataFrame):\n",
    "    i = 0\n",
    "    for i in range(1000):  \n",
    "        change = tzDF[\"measurements_height\"].iloc[i+1:i+21].to_frame().transpose()\n",
    "        #Names of the columns\n",
    "        change.columns = [\"history_1\",\"history_2\",\"history_3\",\"history_4\",\"history_5\",\n",
    "                          \"history_6\",\"history_7\",\"history_8\",\"history_9\",\"history_10\",\n",
    "                          \"history_11\",\"history_12\",\"history_13\",\"history_14\",\"history_15\",\n",
    "                          \"history_16\",\"history_17\",\"history_18\",\"history_19\",\"history_20\"]\n",
    "        change.index = [i]\n",
    "        empty_list.append(change)\n",
    "    return empty_list\n",
    "# List of all columns values \n",
    "tzDF_list = make_list(tzDF[\"measurements_height\"])\n",
    "tzDF_two_list = make_list(tz2DF[\"measurements_height\"])\n",
    "\n",
    "#List iteration to combine all elements in list\n",
    "finalDF = pd.concat([m for m in tzDF_list])\n",
    "finalDF_two =  pd.concat([m for m in tzDF_two_list])\n",
    "finalDF_two\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Increased data points for linear regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below we will take the freshly made data frame finalDF with 200 rows x 20 columns, put the data into a linear regression utilizing the train test split module from the sci-kit library. The test sizes of 35, 55, and 75 are used for variability. The x is all of the rows in the new data frame and columns 1-19, while the or output is all of the rows and the 19th column that we are predicting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 35% Test Data demenstration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def make_prediction(DataFrame,number): \n",
    "    input_Data = finalDF.iloc[:,0:19]\n",
    "    target_Data = finalDF.iloc[:,19]\n",
    "    lm = LinearRegression()\n",
    "    lm.fit(input_Data,target_Data)\n",
    "    y_pred1 = lm.predict(input_Data)\n",
    "    plt.scatter(target_Data,y_pred1)\n",
    "    plt.xlabel(\"target height(m)\")\n",
    "    plt.ylabel(\"predicted height(m)\")\n",
    "    plt.title(\"Target vs Predicted\")\n",
    "    plt.scatter\n",
    "    # x axis is actual height and y is what lm model is predicting in scatter\n",
    "    X_train, X_test, y_train, y_test = train_test_split(input_Data, target_Data,test_size = number, random_state=50)\n",
    "    model = lm.fit(X_train,y_train)\n",
    "    prediction = lm.predict(X_test)\n",
    "    #Series of the difference of the test the leng\n",
    "    error_Series = y_test-prediction\n",
    "    #average\n",
    "    average_Difference = sum(error_Series/len(error_Series))\n",
    "    print(round(average_Difference,4))\n",
    "\n",
    "make_prediction(tzDF,0.55)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When y_test-y_prediciton, we get an average difference of -0.0003 from the model's actual and predicted values in the above cell. From the outliers in the plot, we can view the noise graphically."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 55% Test Data demonstration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function Call for 55% test data dem\n",
    "make_prediction(tzDF,0.55)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the above cell, when y_test-y_prediciton, we get an average difference of -0.00030778063659289 from the model's actual and predicted values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 75% Test Data demonstration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function Call for 75% test data dem\n",
    "make_prediction(tzDF,0.75)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the above cell when y_test-y_prediciton we get an average difference of -0.0001 from the model's actual and predicted values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using one day's data to predict a different days data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The demonstrations below utilize the second data frame now named finalDF_two. The December 16, 2020 dates data will be trained to predict the April 16, 2020 dates data. The test size increases by ten percent in the range of 20-90 for variability. We will be using the Mean Squared value that shows the error for linear regression to view the model's accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction from 10% test size data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def different_Day_Prediction(DataFrame,number):\n",
    "    DECEMBER_SERIES_X  = finalDF.iloc[:,0:19]\n",
    "    DECEMBER_SERIES_Y = finalDF.iloc[:,19]\n",
    "    APRIL_SERIES_X = finalDF_two.iloc[:,0:19]\n",
    "    APRIL_SERIES_Y= finalDF_two.iloc[:,19]\n",
    "    lm.fit(DECEMBER_SERIES_X,DECEMBER_SERIES_Y)\n",
    "    y_pred1 = lm.predict(DECEMBER_SERIES_X)\n",
    "    #train_test_split_module\n",
    "    X_train, X_test, y_train, y_test = train_test_split(DECEMBER_SERIES_X, APRIL_SERIES_Y, test_size=number,random_state=50)\n",
    "    model = lm.fit(X_train,y_train)\n",
    "    model.intercept_\n",
    "    model.coef_\n",
    "    #setting the prediciton variable \n",
    "    APRIL_PREDICTION = model.predict(APRIL_SERIES_X)\n",
    "    #50 bins were picked for all of the models below\n",
    "    #Distrubution of Errors pred vs actual\n",
    "    plt.hist(APRIL_PREDICTION-APRIL_SERIES_Y,bins = 50,color =\"black\")\n",
    "    plt.xlabel(\"meters(m)\")\n",
    "    plt.ylabel(\"distribution\")\n",
    "    plt.title(\"Distrbution of predicted minus actual\")\n",
    "    plt.show()\n",
    "    #Displaying Mean Squared Error\n",
    "    twenty_MSE = mean_squared_error(APRIL_SERIES_Y,APRIL_PREDICTION)\n",
    "    print(\"The mean squared error for this model is\",round(twenty_MSE,4),\"%.\")\n",
    "\n",
    "different_Day_Prediction(tzDF,.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction from 20% test size data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the cell below, december_Series_X and decemeber_Series_Y will be the data that is used to train the model. April_Series_X and april_Series_Y will be the different days and data that we predict it points from. We will display a histogram to view the error distribution between the actual and predicted data as well as "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "different_Day_Prediction(tzDF,.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the above cell, the histogram displayed displays the error from the prediction minus the series. Most of the distribution of the data is centered around zero, indicating the performance of the model. The MSE is extremely low, showing the accuracy of the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction from 30% test size data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the cell below our we will be inputting in 30% test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "different_Day_Prediction(tzDF,.3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction from 40% test size data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "different_Day_Prediction(tzDF,.4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction from 50% test size data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "different_Day_Prediction(tzDF,.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction from 60% test size data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "different_Day_Prediction(tzDF,.6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction from 70% test size data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "different_Day_Prediction(tzDF,.7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction from 80% test size data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "different_Day_Prediction(tzDF,.8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction from 90% test size data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "different_Day_Prediction(tzDF,.9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The mean square error for the above models is deficient, showing the success of inputting in height data for one day and predicting height data from another day."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# References\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Daniels, Mike, Branko Kerkez, V. Chandrasekar, Sara Graves, D. Sarah Stamps, Charles Martin, Aaron Botnick, Michael Dye, Ryan Gooch, Josh Jones, Ken Keiser, Matthew Bartos, Thaovy Nguyen, Robyn Collins, Sophia Chen, Terrie Yang, Abbi Devins-Suresh (2016). Cloud-Hosted Real-time Data Services for the Geosciences (CHORDS) software (Version 1.0.1). UCAR/NCAR - EarthCube. https://doi.org/10.5065/d6v1236q\n",
    "\n",
    "Kerkez, Branko, Michael Daniels, Sara Graves, V. Chandrasekar, Ken Keiser, Charlie Martin, Michael Dye, Manil Maskey, and Frank Vernon. \"Cloud Hosted Real‐time Data Services for the Geosciences (CHORDS).\" (2016), doi: 10.1002/gdj3.36. \n",
    "\n",
    "GeÌ ron, Aureì lien. 2019. Hands-on Machine Learning with Scikit-Learn, Keras and TensorFlow: Concepts, Tools, and Techniques to Build Intelligent Systems. 2nd ed. CA 95472: O’Reilly.\n",
    "\n",
    "Stamps, D. S., Saria, E., Ji, K. H., Jones, J. R., Ntambila, D., Daniels, M. D., &amp; Mencin, D. (2016). <i>Real-time data from the Tanzania Volcano Observatory at the Ol Doinyo Lengai volcano in Tanzania (TZVOLCANO).</i> UCAR/NCAR - EarthCube. https://doi.org/10.5065/D6P849BM"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "248.172px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
