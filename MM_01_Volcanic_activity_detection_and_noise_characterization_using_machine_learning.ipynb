{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EC 01 Volcanic activity detection and noise characterization using machine learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Linear regression prediction of height point."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Author(s)\n",
    "\n",
    "\n",
    "- Author1 = {\"name\": \"Myles Mason\", \"affiliation\": \"Virginia Tech\", \"email\": \"mylesm18@vt.edu\", \"orcid\": \"0000-0002-8811-8294\"}\n",
    "- Author2 = {\"name\": \"John Wenskovitch\", \"affiliation\": \"Virginia Tech\", \"email\": \"jw87@vt.edu\", \"orcid\": \"0000-0002-0573-6442\"}\n",
    "- Author3 = {\"name\": \"D. Sarah Stamps\", \"affiliation\": \"Virginia Tech\", \"email\": \"dstamps@vt.edu\",\"orcid\": \"0000-0002-3531-1752\"}\n",
    "- Author4 = {\"name\": \"Joshua Robert Jones\", \"affiliation\": \"Virginia Tech\", \"email\": \"joshj55@vt.edu\", \"orcid\": \"0000-0002-6078-4287\"}\n",
    "- Author5 = {\"name\": \"Mike Dye\", \"affiliation\": \"Unaffiliated\", \"email\": \"mike@mikedye.com\", \"orcid\": \" 0000-0003-2065-870X\"}\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Purpose\n",
    "\n",
    "This Jupyter notebook explores methods towards characterizing noise and eventually predicting volcanic activity for [Ol Doinyo Lengai](http://tzvolcano.chordsrt.com)\n",
    " (an active volcano in Tanzania) with machine learning. Machine learning is a powerful tool that enables the automatization of complex mathematical and analytical models. In this Jupyter notebook, the components are time, height, latitude, and longitude. The predicted component values are the following heights. This project uses Global Navigation Satellite System (GNSS) data from the EarthCube CHORDS portal TZVOLCANO (Stamps et al. 2016; Daniels et al., 2016; Kerkez et al., 2016), which is the online interface for obtaining open-access real-time positioning data collected around Ol Doinyo Lengai(http://tzvolcano.chordsrt.com). The bulk of the project is the exploration of the data and later prediction of height points. The station that this project analyzes is OLO1 for days 12/16/2020 and 04/16/2021.  \n",
    "\n",
    "## Technical contributions\n",
    "- The training of the models and analysis uses basic linear algebra and statistics \n",
    "- The main libraries used (NumPy and pandas) are both libraries for data manipulation and linear algebra \n",
    "- The CHORDS site linked above is the location of the data and the interface of [CHORDS](http://tzvolcano.chordsrt.com)\n",
    "- Implementation of Linear Regression for prediction on time-series data\n",
    "\n",
    "## Methodology\n",
    "The desired data was imported and selected. Pre-processing and cleaning of the data occurred. The information was then visualized for better analysis along with statistical metrics running. Finally, linear regression models were built and analyzed for the data.\n",
    "\n",
    "\n",
    "## Results\n",
    "This notebook explored predicting height data from the TZVOLCANO CHORDS portal using Linear Regression from different days. It also evaluates how much test data is needed to best predict height data. We find that having 10% test data yields the best results for predictions with the Mean Squared Error of 8.325e-5% . For predictions from data inputted and predicted from a single day we find the 75% test data yields the best results with an average error of -1.074e-4 meters.\n",
    "\n",
    "## Funding\n",
    "\n",
    "- Award1 = {\"agency\": \"National Science Foundation EarthCube Program\", \"award_code\": \"1639554\", \"award_URL\": https://www.nsf.gov/awardsearch/showAward?AWD_ID=1639554&HistoricalAwards=false }\n",
    "- Award2 = {\"agency\": \"Virginia Tech Academy of Integrated Sciences Hamlett Undergraduate Research Award\", \"award_code\": \"44672\", \"award_URL\": \"\"}\n",
    "\n",
    "## Keywords\n",
    "\n",
    "\n",
    "keywords=[\"tzDF\", \"Linear Regression\", \"Concat\", \"Transpose\",\"Mean Squared Error(MSE)\"]\n",
    "\n",
    "## Citation\n",
    "Mason, Myles, John Wenskovitch, D. Sarah Stamps, Joshua Robert Jones,  Mike Dye (2021), EC_01_Volcanic_activity_detection_and_noise_characterization_using_machine learning, EarthCube Annual Meeting.\n",
    "\n",
    "## Suggested next steps\n",
    "The next step for this notebook will be increasing the inputs for the prediction model. More analysis of the noise will be crucial in the next step for further volcanic activity prediction. They are explicitly generating synthetic data that will mimic volcanic activity.\n",
    "## Acknowledgements\n",
    "- Virginia Tech Department of Geosciences \n",
    "- Alice and Luther Hamlet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup\n",
    "\n",
    "## Library import\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data manipulation\n",
    "import pandas as pd\n",
    "import json\n",
    "import numpy as np\n",
    "import math\n",
    "from datetime import datetime as dt\n",
    "\n",
    "\n",
    "# Visualizations\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Modeling\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parameter definitions\n",
    "- 'tzDF': intial dataframe contains 12/06/2020 data \n",
    "- 'tz2DF': secondary dataframe contains 04/16/2021 data\n",
    "- 'ONE_THROUGH_TWENTY': array of values in tzDF[\"measurements_height\"] used for prediction\n",
    "- 'TWO_THROUGH_TWENTY_ONE':array of values in tzDF[\"measurements_height\"] used for prediction\n",
    "- 'THREE_THROUGH_TWENTY_TWO': array of values in tzDF[\"measurements_height\"] used for prediction\n",
    "- 'FOUR_THROUGH_TWENTY_THREE': array of values in tzDF[\"measurements_height\"] used for prediction\n",
    "- 'DECEMBER_SERIES_X': inputted height for 12/16/2020 data for Linear Regression \n",
    "- 'DECEMBER_SERIES_Y': target height data for 12/16/2020\n",
    "- 'APRIL_SERIES_X': inputted height data for 04/16/2021\n",
    "- 'APRIL_SERIES_Y': target height data for 04/16/2021\n",
    "- 'APRIL_PREDICTION': Predicted values from the two days\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Data import\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The CHORDS notebook portal is where the data is acessed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'type': 'FeatureCollection',\n",
       " 'features': [{'type': 'Feature',\n",
       "   'geometry': {'type': 'Point', 'coordinates': [35.8714, -2.754, 1483.0]},\n",
       "   'properties': {'project': 'TZVOLCANO GNSS Network',\n",
       "    'affiliation': 'Virginia Tech, Ardhi University, KIGAM',\n",
       "    'doi': 'https://doi.org/10.5065/D6P849BM',\n",
       "    'doi_citation': 'https://citation.crosscite.org/?doi=10.5065/D6P849BM',\n",
       "    'chords_version': '1.1.0-rc1',\n",
       "    'chords_version_sha': 'ed6ed8f',\n",
       "    'site': 'Border Fault',\n",
       "    'site_id': 3,\n",
       "    'instrument': 'OLO3',\n",
       "    'instrument_id': 2,\n",
       "    'sensor_id': None,\n",
       "    'variables': [{'name': 'Latitude',\n",
       "      'shortname': 'lat',\n",
       "      'measured_property': 'Latitude',\n",
       "      'measured_property_definition': \"The angular distance north or south from the earth''s equator measured through 90 degrees.\",\n",
       "      'measured_property_source': 'SensorML',\n",
       "      'measured_property_url': 'http://sensorml.com/ont/swe/property/Latitude',\n",
       "      'units_name': 'percent',\n",
       "      'units_abbreviation': '%',\n",
       "      'units_type': 'Dimensionless',\n",
       "      'units_source': 'CUAHSI'},\n",
       "     {'name': 'Longitude',\n",
       "      'shortname': 'lon',\n",
       "      'measured_property': 'Longitude',\n",
       "      'measured_property_definition': \"The arc or portion of the Earth''s equator intersected between the meridian of a given place and the prime meridian.\",\n",
       "      'measured_property_source': 'SensorML',\n",
       "      'measured_property_url': 'http://sensorml.com/ont/swe/property/Longitude',\n",
       "      'units_name': 'percent',\n",
       "      'units_abbreviation': '%',\n",
       "      'units_type': 'Dimensionless',\n",
       "      'units_source': 'CUAHSI'},\n",
       "     {'name': 'Height',\n",
       "      'shortname': 'height',\n",
       "      'measured_property': 'Average Terrain Height',\n",
       "      'measured_property_definition': 'Average scene elevation.',\n",
       "      'measured_property_source': 'SensorML',\n",
       "      'measured_property_url': 'http://sensorml.com/ont/swe/property/AverageTerrainHeight',\n",
       "      'units_name': 'percent',\n",
       "      'units_abbreviation': '%',\n",
       "      'units_type': 'Dimensionless',\n",
       "      'units_source': 'CUAHSI'}],\n",
       "    'data': [],\n",
       "    'timestamps_in_feature': 0,\n",
       "    'measurements_in_feature': 0}}]}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Import for JSON files for manipulation\n",
    "''' Both files are station one, but the first date is December 16,2020\n",
    "    while the second date is April 16 2021.\n",
    "'''\n",
    "with open('OLO1_12_16_20.geojson', 'r', encoding=\"utf-8\") as infile:\n",
    "    tzList = json.load(infile)\n",
    "\n",
    "with open('OLO1_4_16_21.geojson', 'r', encoding=\"utf-8\") as infile:\n",
    "    tz2List = json.load(infile)\n",
    "\n",
    "#OLO3 imports\n",
    "#Import for JSON files for manipulation\n",
    "\n",
    "with open('OLO3_2_16_21.geojson', 'r', encoding=\"utf-8\") as infile:\n",
    "    tz3List = json.load(infile)\n",
    "\n",
    "with open('OLO3_5_16_21.geojson', 'r', encoding=\"utf-8\") as infile:\n",
    "    tz4List = json.load(infile)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data processing and analysis\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Convert both JSON's into  a partially-flattened pandas DataFrame   \n",
    "tzDF =  pd.json_normalize(tzList[\"features\"][0][\"properties\"][\"data\"], sep='_')\n",
    "tz2DF =  pd.json_normalize(tz2List[\"features\"][0][\"properties\"][\"data\"], sep='_')\n",
    "tz3DF = pd.json_normalize(tz3List[\"features\"][0][\"properties\"][\"data\"], sep='_')\n",
    "#tz4DF = pd.json_normalize(tz4List[\"features\"][0][\"properties\"][\"data\"], sep='_')\n",
    "#Overview of numerical elements of the datat\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the .describe call looking at our data frame, there does not seem to be much variation in columns measurements_lat and measurements_lon because the DataFrame is from a single day. The reason for choosing to explore the measurements_height column in this notebook is because of the hypothesis that the surface will uplift if there is magma reservoir inflation or the surface will subside if there is magma reservior deflation. From viewing the data frame, the time column is in a time series form with extra separator variables \"T\" and \"Z\" in the following two cells; we will convert the timestamp column into an integer form for easy manipulation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Function to convert timestamp column of tzDF and tz2DF from time series to an integer for easy manipulation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'time'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m/opt/anaconda3/lib/python3.8/site-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   2894\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2895\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcasted_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2896\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'time'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-8ea34d7b026c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mtz2DF\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"timeconvert\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtz2DF\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"time\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeconvertfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0mtz3DF\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"timeconvert\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtz3DF\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"time\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeconvertfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0mtz4DF\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"timeconvert\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtz4DF\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"time\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeconvertfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/opt/anaconda3/lib/python3.8/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   2900\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnlevels\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2901\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_multilevel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2902\u001b[0;31m             \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2903\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mis_integer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2904\u001b[0m                 \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.8/site-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   2895\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcasted_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2896\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2897\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2898\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2899\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtolerance\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'time'"
     ]
    }
   ],
   "source": [
    "def timeconvertfunc(timestamp):\n",
    " #The format of tzDF[\"measurments_height] is in string form and timeseries so this method make it an integer\"\n",
    "  ts = pd.Timestamp(timestamp, tz=None).to_pydatetime()\n",
    "  ts = 3600*ts.hour + 60*ts.minute + ts.second\n",
    "\n",
    "  return ts\n",
    "#Applying above method to the two data frames\n",
    "tzDF[\"timeconvert\"] = tzDF[\"time\"].apply(timeconvertfunc)\n",
    "tz2DF[\"timeconvert\"] = tz2DF[\"time\"].apply(timeconvertfunc)\n",
    "tz3DF[\"timeconvert\"] = tz3DF[\"time\"].apply(timeconvertfunc)\n",
    "tz4DF[\"timeconvert\"] = tz4DF[\"time\"].apply(timeconvertfunc)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualization of basic statistics from measurements_height and linear regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the code block below the four series objects are partitions of the measurements_height column in the tzDF DataFrame. We create these partitions to feed into a linear regression model for predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Series from the first Data Frame\n",
    "\n",
    "ONE_THROUGH_TWENTY = tzDF[\"measurements_height\"].loc[1:21].values.reshape(-1,1)\n",
    "TWO_THROUGH_TWENTY_ONE = tzDF[\"measurements_height\"].loc[2:22].values.reshape(-1,1)\n",
    "THREE_THROUGH_TWENTY_TWO =tzDF[\"measurements_height\"].loc[3:23].values.reshape(-1,1)\n",
    "FOUR_THROUGH_TWENTY_FOUR = tzDF[\"measurements_height\"].loc[4:24].values.reshape(-1,1)\n",
    "\n",
    "#Linear Regression model on columns 1-20 and 2-21\n",
    "\n",
    "lm = LinearRegression()\n",
    "lm.fit(ONE_THROUGH_TWENTY ,TWO_THROUGH_TWENTY_ONE)\n",
    "y_pred = lm.predict(ONE_THROUGH_TWENTY)\n",
    "plt.xlabel(\"actual height(meters)\")\n",
    "plt.ylabel(\"predicted height(meters)\")\n",
    "plt.title(\"actual height vs predicted height\")\n",
    "plt.scatter(ONE_THROUGH_TWENTY,TWO_THROUGH_TWENTY_ONE)\n",
    "plt.plot(ONE_THROUGH_TWENTY,y_pred,color=\"red\")\n",
    "plt.show()\n",
    "\n",
    "#Method to round for sigfigs\n",
    "def to_precision(x,p):\n",
    "    x = float(x)\n",
    "    if x == 0.:\n",
    "        return \"0.\" + \"0\"*(p-1)\n",
    "    out = []\n",
    "    if x < 0:\n",
    "        out.append(\"-\")\n",
    "        x = -x\n",
    "    e = int(math.log10(x))\n",
    "    tens = math.pow(10, e - p + 1)\n",
    "    n = math.floor(x/tens)\n",
    "    if n < math.pow(10, p - 1):\n",
    "        e = e -1\n",
    "        tens = math.pow(10, e - p+1)\n",
    "        n = math.floor(x / tens)\n",
    "    if abs((n + 1.) * tens - x) <= abs(n * tens -x):\n",
    "        n = n + 1\n",
    "    if n >= math.pow(10,p):\n",
    "        n = n / 10.\n",
    "        e = e + 1\n",
    "    m = \"%.*g\" % (p, n)\n",
    "    if e < -2 or e >= p:\n",
    "        out.append(m[0])\n",
    "        if p > 1:\n",
    "            out.append(\".\")\n",
    "            out.extend(m[1:p])\n",
    "        out.append('e')\n",
    "        if e > 0:\n",
    "            out.append(\"+\")\n",
    "        out.append(str(e))\n",
    "    elif e == (p -1):\n",
    "        out.append(m)\n",
    "    elif e >= 0:\n",
    "        out.append(m[:e+1])\n",
    "        if e+1 < len(m):\n",
    "            out.append(\".\")\n",
    "            out.extend(m[e+1:])\n",
    "    else:\n",
    "        out.append(\"0.\")\n",
    "        out.extend([\"0\"]*-(e+1))\n",
    "        out.append(m)\n",
    "\n",
    "    return \"\".join(out)\n",
    "print(\"The b value is\",to_precision(lm.coef_[0][0],4))\n",
    "\n",
    "print(\"The m value is\",to_precision(lm.intercept_[0],4))\n",
    "\n",
    "\n",
    "print(\"The R^2 value is\",to_precision(lm.score(ONE_THROUGH_TWENTY,TWO_THROUGH_TWENTY_ONE),4),)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the above model using the first through the twentieth column and second, through the twenty-second column, we yield a Coefficient of Correlation (R^2) value of about 0.73, which shows a positive correlation between the two inputs. So about 37% of the variation is residing in the residual."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TWO_THROUGH_TWENTY_ONE and THREE_THROUGH_TWENTY_TWO "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "lm = LinearRegression()\n",
    "lm.fit(TWO_THROUGH_TWENTY_ONE ,THREE_THROUGH_TWENTY_TWO)\n",
    "y_pred1 = lm.predict(TWO_THROUGH_TWENTY_ONE)\n",
    "plt.scatter(TWO_THROUGH_TWENTY_ONE,THREE_THROUGH_TWENTY_TWO)\n",
    "plt.plot(TWO_THROUGH_TWENTY_ONE,y_pred1,color=\"green\")\n",
    "plt.xlabel(\"actual height(meters)\")\n",
    "plt.ylabel(\"predicted height(meters)\")\n",
    "plt.title(\"actual height vs predicted height\")\n",
    "plt.show()\n",
    "\n",
    "\n",
    "print(\"This is the b value\",to_precision(lm.coef_[0][0],4))\n",
    "print(\"This is the m value\",to_precision(lm.intercept_[0],4))\n",
    "print(\"This is the R^2\",to_precision(lm.score(TWO_THROUGH_TWENTY_ONE,THREE_THROUGH_TWENTY_TWO),4))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the above model using the first through the twentieth column and the second through the twenty-second column, we yield a Coefficient of Correlation (R^2) value of about 0.72, which shows a positive correlation between the two inputs. So about 38% of the variation is residing in the residual."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Regression from single day"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code chunk uses  Linear Regression (specifically with rows of height measurement) for  one_Through_Twenty,two_Through_Twenty_One, and \n",
    "three_Through_Twenty_Two. The data frame used for the model is tzDF, and we display the predicted values versus actual values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Set up series object for partions in the dataframe\n",
    "ONE_THROUGH_TWENTY = tzDF[\"measurements_height\"].loc[1:21]\n",
    "TWO_THROUGH_TWENTY_ONE= tzDF[\"measurements_height\"].loc[2:22]\n",
    "THREE_THROUGH_TWENTY_TWO =tzDF[\"measurements_height\"].loc[3:23]\n",
    "FOUR_THROUGH_TWENTY_THREE = tzDF[\"measurements_height\"].loc[4:24]\n",
    "\n",
    "#Renaming the series objects\n",
    "ONE_THROUGH_TWENTY.rename({\"measurements_height\":\"w\"},axis =1,inplace=True)\n",
    "TWO_THROUGH_TWENTY_ONE.rename({\"measurements_height\":\"x\"},axis =1,inplace=True)\n",
    "THREE_THROUGH_TWENTY_TWO.rename({\"measurements_height\":\"y\"},axis =1,inplace=True)\n",
    "FOUR_THROUGH_TWENTY_THREE.rename({\"measurements_height\":\"z\"},axis =1,inplace=True)\n",
    "\n",
    "#Concating the series objects to one dataframe, result_DF\n",
    "result_DF = pd.concat([ONE_THROUGH_TWENTY,TWO_THROUGH_TWENTY_ONE,THREE_THROUGH_TWENTY_TWO,FOUR_THROUGH_TWENTY_THREE],axis=1)\n",
    "result_DF\n",
    "\n",
    "#Modefying dataframe by shifting the coulmns up \n",
    "result_DF.iloc[:,1] = result_DF.iloc[:,1].shift(-1)\n",
    "result_DF.iloc[:,2] = result_DF.iloc[:,2].shift(-2)\n",
    "result_DF.iloc[:,3] = result_DF.iloc[:,3].shift(-3)\n",
    "\n",
    "#Aligning all coulmns of the data frame together\n",
    "result_DF = result_DF.dropna()\n",
    "result_DF = result_DF.transpose()\n",
    "result_DF\n",
    "\n",
    "#Linear Regression on all rows and 1-19 coulmns \n",
    "lm = LinearRegression()\n",
    "x = result_DF.iloc[:,0:20]\n",
    "y = result_DF.iloc[:,20]\n",
    "lm.fit(x,y)\n",
    "y_pred1 = lm.predict(x)\n",
    "plt.scatter(y,y_pred1)\n",
    "plt.xlabel(\"actual height(m)\")\n",
    "plt.ylabel(\"predicted height(m)\")\n",
    "plt.title(\"actual height vs predicted height\")\n",
    "plt.show()\n",
    "#REMOVE CODE\n",
    "#print(\"This is the b value\",lm.coef_,)\n",
    "#print(\"This is the m value\",lm.intercept_)\n",
    "#print(\"This is the R^2\",lm.score(x,y))\n",
    "print(y-y_pred1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the above graph, all points are predicted as the values overlap graphically with a b value of 0.0194. The height difference is zero. We will now increase the sample size of the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Method to increase sample size for the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The method below takes in a data frame, goes into measurement height, and gets the first through nineteenth values in the height data frame. The first and nineteenth values are increased by one thousand times. The Data frame is transposed into columns then added to an empty list to be concatenated into a data frame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Making an empty list to store coulmn values\n",
    "empty_list = []\n",
    "def make_list(DataFrame):\n",
    "    i = 0\n",
    "    for i in range(1000):  \n",
    "        change = tzDF[\"measurements_height\"].iloc[i+1:i+21].to_frame().transpose()\n",
    "        #Names of the columns\n",
    "        change.columns = [\"history_1\",\"history_2\",\"history_3\",\"history_4\",\"history_5\",\n",
    "                          \"history_6\",\"history_7\",\"history_8\",\"history_9\",\"history_10\",\n",
    "                          \"history_11\",\"history_12\",\"history_13\",\"history_14\",\"history_15\",\n",
    "                          \"history_16\",\"history_17\",\"history_18\",\"history_19\",\"history_20\"]\n",
    "        change.index = [i]\n",
    "        empty_list.append(change)\n",
    "    return empty_list\n",
    "# List of all columns values \n",
    "tzDF_list = make_list(tzDF[\"measurements_height\"])\n",
    "tzDF_two_list = make_list(tz2DF[\"measurements_height\"])\n",
    "tzDF_three_list = make_list(tz3DF[\"measurements_height\"])\n",
    "#tzDF_four_list = make_list(tz4DF[\"measurements_height\"])\n",
    "#tzDF_four_list = make_list(tz4DF[\"measurements_height\"])\n",
    "\n",
    "#List iteration to combine all elements in list\n",
    "finalDF = pd.concat([m for m in tzDF_list])\n",
    "finalDF_two =  pd.concat([m for m in tzDF_two_list])\n",
    "finalDF_three = pd.concat([m for m in tzDF_three_list])\n",
    "#finalDF_four = pd.concat([m for m in tzDF_four_list])\n",
    "tz3DF.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Increased data points for linear regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below we will take the freshly made data frame finalDF with 200 rows x 20 columns, put the data into a linear regression utilizing the train test split module from the sci-kit library. The test sizes of 35, 55, and 75 are used for variability. The x is all of the rows in the new data frame and columns 1-19, while the or output is all of the rows and the 19th column that we are predicting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 35% Test Data demonstration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def make_prediction(DataFrame,number): \n",
    "    input_Data = finalDF.iloc[:,0:19]\n",
    "    target_Data = finalDF.iloc[:,19]\n",
    "    lm = LinearRegression()\n",
    "    lm.fit(input_Data,target_Data)\n",
    "    y_pred1 = lm.predict(input_Data)\n",
    "    plt.scatter(target_Data,y_pred1)\n",
    "    plt.xlabel(\"target height(m)\")\n",
    "    plt.ylabel(\"predicted height(m)\")\n",
    "    plt.title(\"Target vs Predicted\")\n",
    "    plt.scatter\n",
    "    # x axis is actual height and y is what lm model is predicting in scatter\n",
    "    X_train, X_test, y_train, y_test = train_test_split(input_Data, target_Data,test_size = number, random_state=50)\n",
    "    model = lm.fit(X_train,y_train)\n",
    "    prediction = lm.predict(X_test)\n",
    "    #Series of the difference of the test the leng\n",
    "    error_Series = y_test-prediction\n",
    "    #average\n",
    "    average_Difference = sum(error_Series/len(error_Series))\n",
    "    #print(average_Difference)\n",
    "    print(to_precision(average_Difference,4))\n",
    "\n",
    "make_prediction(tzDF,0.35)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When y_test-y_prediciton, we get an average difference of -0.0003 from the model's actual and predicted values in the above cell. From the outliers in the plot, we can view the noise graphically."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 55% Test Data demonstration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function Call for 55% test data dem\n",
    "make_prediction(tzDF,0.55)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the above cell, when y_test-y_prediciton, we get an average difference of -0.00030778063659289 from the model's actual and predicted values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 75% Test Data demonstration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function Call for 75% test data dem\n",
    "make_prediction(tzDF,0.75)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the above cell when y_test-y_prediciton we get an average difference of -0.0001 from the model's actual and predicted values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using one day's data to predict a different day's data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The demonstrations below utilize the second data frame now named finalDF_two. The December 16, 2020 dates data will be trained to predict the April 16, 2020 dates data. The test size increases by ten percent in the range of 20-90 for variability. We will be using the Mean Squared value that shows the error for linear regression to view the model's accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction from 10% test size data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def different_Day_Prediction(DataFrame,number):\n",
    "    DECEMBER_SERIES_X  = finalDF.iloc[:,0:19]\n",
    "    DECEMBER_SERIES_Y = finalDF.iloc[:,19]\n",
    "    APRIL_SERIES_X = finalDF_two.iloc[:,0:19]\n",
    "    APRIL_SERIES_Y= finalDF_two.iloc[:,19]\n",
    "    lm.fit(DECEMBER_SERIES_X,DECEMBER_SERIES_Y)\n",
    "    y_pred1 = lm.predict(DECEMBER_SERIES_X)\n",
    "    #train_test_split_module\n",
    "    X_train, X_test, y_train, y_test = train_test_split(DECEMBER_SERIES_X, APRIL_SERIES_Y, test_size=number,random_state=50)\n",
    "    model = lm.fit(X_train,y_train)\n",
    "    model.intercept_\n",
    "    model.coef_\n",
    "    #setting the prediciton variable \n",
    "    APRIL_PREDICTION = model.predict(APRIL_SERIES_X)\n",
    "    #50 bins were picked for all of the models below\n",
    "    #Distrubution of Errors pred vs actual\n",
    "    plt.hist(APRIL_PREDICTION-APRIL_SERIES_Y,bins = 50,color =\"black\")\n",
    "    plt.xlabel(\"meters(m)\")\n",
    "    plt.ylabel(\"distribution\")\n",
    "    plt.title(\"Distrbution of predicted minus actual\")\n",
    "    plt.show()\n",
    "    #Displaying Mean Squared Error\n",
    "    twenty_MSE = mean_squared_error(APRIL_SERIES_Y,APRIL_PREDICTION)\n",
    "    print(\"The mean squared error for this model is\",to_precision(twenty_MSE,4),\"%.\")\n",
    "\n",
    "different_Day_Prediction(tzDF,.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction from 20% test size data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the cell below, december_Series_X and decemeber_Series_Y will be the data that is used to train the model. April_Series_X and april_Series_Y will be the different days and data that we predict it points from. We will display a histogram to view the error distribution between the actual and predicted data as well as "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "different_Day_Prediction(tzDF,.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the above cell, the histogram displayed displays the error from the prediction minus the series. Most of the distribution of the data is centered around zero, indicating the performance of the model. The MSE is extremely low, showing the accuracy of the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction from 30% test size data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the cell below our we will be inputting in 30% test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "different_Day_Prediction(tzDF,.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def different_Day_Prediction(DataFrame,number):\n",
    "    DECEMBER_SERIES_X  = finalDF.iloc[:,0:19]\n",
    "    DECEMBER_SERIES_Y = finalDF.iloc[:,19]\n",
    "    APRIL_SERIES_X = finalDF_two.iloc[:,0:19]\n",
    "    APRIL_SERIES_Y= finalDF_two.iloc[:,19]\n",
    "    lm.fit(DECEMBER_SERIES_X,DECEMBER_SERIES_Y)\n",
    "    y_pred1 = lm.predict(DECEMBER_SERIES_X)\n",
    "    #train_test_split_module\n",
    "    X_train, X_test, y_train, y_test = train_test_split(DECEMBER_SERIES_X, APRIL_SERIES_Y, test_size=number,random_state=50)\n",
    "    model = lm.fit(X_train,y_train)\n",
    "    model.intercept_\n",
    "    model.coef_\n",
    "    #setting the prediciton variable \n",
    "    APRIL_PREDICTION = model.predict(APRIL_SERIES_X)\n",
    "    #50 bins were picked for all of the models below\n",
    "    #Distrubution of Errors pred vs actual\n",
    "    plt.hist(APRIL_PREDICTION-APRIL_SERIES_Y,bins = 50,color =\"black\")\n",
    "    plt.xlabel(\"meters(m)\")\n",
    "    plt.ylabel(\"distribution\")\n",
    "    plt.title(\"Distrbution of predicted minus actual\")\n",
    "    plt.show()\n",
    "    #Displaying Mean Squared Error\n",
    "    twenty_MSE = mean_squared_error(APRIL_SERIES_Y,APRIL_PREDICTION)\n",
    "    print(\"The mean squared error for this model is\",to_precision(twenty_MSE,4),\"%.\")\n",
    "\n",
    "different_Day_Prediction(tzDF,.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction from 40% test size data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "different_Day_Prediction(tzDF,.4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction from 50% test size data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "different_Day_Prediction(tzDF,.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction from 60% test size data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "different_Day_Prediction(tzDF,.6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction from 70% test size data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "different_Day_Prediction(tzDF,.7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction from 80% test size data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "different_Day_Prediction(tzDF,.8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction from 90% test size data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "different_Day_Prediction(tzDF,.9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The mean square error for the above models is deficient, showing the success of inputting in height data for one day and predicting height data from another day."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With this notebook we explored the use of machine learning for predictive analytics applied to vertical surface motions at an active volcano in Tanzania, Ol Doinyo Lengai. When training a model on one day and using that model to predict height values from another day, our lowest error was 8.325e-5 %. The future implications for this project and data are important because Ol Doinyo Lengai future eruption could cause distress for not only Tanzania, but other countries surrounding Tanzania. The ability to predict volcanic activity will be a valuable contribution to volcanic hazards assessment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# References\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Daniels, Mike, Branko Kerkez, V. Chandrasekar, Sara Graves, D. Sarah Stamps, Charles Martin, Aaron Botnick, Michael Dye, Ryan Gooch, Josh Jones, Ken Keiser, Matthew Bartos, Thaovy Nguyen, Robyn Collins, Sophia Chen, Terrie Yang, Abbi Devins-Suresh (2016). Cloud-Hosted Real-time Data Services for the Geosciences (CHORDS) software (Version 1.0.1). UCAR/NCAR - EarthCube. https://doi.org/10.5065/d6v1236q\n",
    "\n",
    "Kerkez, Branko, Michael Daniels, Sara Graves, V. Chandrasekar, Ken Keiser, Charlie Martin, Michael Dye, Manil Maskey, and Frank Vernon. \"Cloud Hosted Real‐time Data Services for the Geosciences (CHORDS).\" (2016), doi: 10.1002/gdj3.36. \n",
    "\n",
    "GeÌ ron, Aureì lien. 2019. Hands-on Machine Learning with Scikit-Learn, Keras and TensorFlow: Concepts, Tools, and Techniques to Build Intelligent Systems. 2nd ed. CA 95472: O’Reilly.\n",
    "\n",
    "Stamps, D. S., Saria, E., Ji, K. H., Jones, J. R., Ntambila, D., Daniels, M. D., &amp; Mencin, D. (2016). <i>Real-time data from the Tanzania Volcano Observatory at the Ol Doinyo Lengai volcano in Tanzania (TZVOLCANO).</i> UCAR/NCAR - EarthCube. https://doi.org/10.5065/D6P849BM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OLO3 Observation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>time</th>\n",
       "      <th>test</th>\n",
       "      <th>measurements_lat</th>\n",
       "      <th>measurements_height</th>\n",
       "      <th>measurements_lon</th>\n",
       "      <th>timeconvert</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2021-02-17T04:09:43Z</td>\n",
       "      <td>false</td>\n",
       "      <td>2.753983</td>\n",
       "      <td>1483.983</td>\n",
       "      <td>35.871398</td>\n",
       "      <td>14983</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2021-02-17T04:09:44Z</td>\n",
       "      <td>false</td>\n",
       "      <td>2.753983</td>\n",
       "      <td>1483.996</td>\n",
       "      <td>35.871398</td>\n",
       "      <td>14984</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2021-02-17T04:09:45Z</td>\n",
       "      <td>false</td>\n",
       "      <td>2.753982</td>\n",
       "      <td>1484.458</td>\n",
       "      <td>35.871397</td>\n",
       "      <td>14985</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2021-02-17T04:09:46Z</td>\n",
       "      <td>false</td>\n",
       "      <td>2.753982</td>\n",
       "      <td>1484.457</td>\n",
       "      <td>35.871397</td>\n",
       "      <td>14986</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2021-02-17T04:09:47Z</td>\n",
       "      <td>false</td>\n",
       "      <td>2.753982</td>\n",
       "      <td>1483.998</td>\n",
       "      <td>35.871397</td>\n",
       "      <td>14987</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1344</th>\n",
       "      <td>2021-02-17T04:59:56Z</td>\n",
       "      <td>false</td>\n",
       "      <td>2.753982</td>\n",
       "      <td>1483.111</td>\n",
       "      <td>35.871396</td>\n",
       "      <td>17996</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1345</th>\n",
       "      <td>2021-02-17T04:59:57Z</td>\n",
       "      <td>false</td>\n",
       "      <td>2.753982</td>\n",
       "      <td>1483.119</td>\n",
       "      <td>35.871396</td>\n",
       "      <td>17997</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1346</th>\n",
       "      <td>2021-02-17T04:59:58Z</td>\n",
       "      <td>false</td>\n",
       "      <td>2.753982</td>\n",
       "      <td>1483.121</td>\n",
       "      <td>35.871396</td>\n",
       "      <td>17998</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1347</th>\n",
       "      <td>2021-02-17T04:59:59Z</td>\n",
       "      <td>false</td>\n",
       "      <td>2.753982</td>\n",
       "      <td>1483.134</td>\n",
       "      <td>35.871396</td>\n",
       "      <td>17999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1348</th>\n",
       "      <td>2021-02-17T05:00:00Z</td>\n",
       "      <td>false</td>\n",
       "      <td>2.753982</td>\n",
       "      <td>1483.132</td>\n",
       "      <td>35.871396</td>\n",
       "      <td>18000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1349 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                      time   test  measurements_lat  measurements_height  \\\n",
       "0     2021-02-17T04:09:43Z  false          2.753983             1483.983   \n",
       "1     2021-02-17T04:09:44Z  false          2.753983             1483.996   \n",
       "2     2021-02-17T04:09:45Z  false          2.753982             1484.458   \n",
       "3     2021-02-17T04:09:46Z  false          2.753982             1484.457   \n",
       "4     2021-02-17T04:09:47Z  false          2.753982             1483.998   \n",
       "...                    ...    ...               ...                  ...   \n",
       "1344  2021-02-17T04:59:56Z  false          2.753982             1483.111   \n",
       "1345  2021-02-17T04:59:57Z  false          2.753982             1483.119   \n",
       "1346  2021-02-17T04:59:58Z  false          2.753982             1483.121   \n",
       "1347  2021-02-17T04:59:59Z  false          2.753982             1483.134   \n",
       "1348  2021-02-17T05:00:00Z  false          2.753982             1483.132   \n",
       "\n",
       "      measurements_lon  timeconvert  \n",
       "0            35.871398        14983  \n",
       "1            35.871398        14984  \n",
       "2            35.871397        14985  \n",
       "3            35.871397        14986  \n",
       "4            35.871397        14987  \n",
       "...                ...          ...  \n",
       "1344         35.871396        17996  \n",
       "1345         35.871396        17997  \n",
       "1346         35.871396        17998  \n",
       "1347         35.871396        17999  \n",
       "1348         35.871396        18000  \n",
       "\n",
       "[1349 rows x 6 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#2/16/21\n",
    "tz3DF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Different day prediction for OLO3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def different_Day_Prediction(DataFrame,number):\n",
    "    DECEMBER_SERIES_X  = finalDF.iloc[:,0:19]\n",
    "    DECEMBER_SERIES_Y = finalDF.iloc[:,19]\n",
    "    APRIL_SERIES_X = finalDF_three.iloc[:,0:19]\n",
    "    APRIL_SERIES_Y= finalDF_three.iloc[:,19]\n",
    "    lm.fit(DECEMBER_SERIES_X,DECEMBER_SERIES_Y)\n",
    "    y_pred1 = lm.predict(DECEMBER_SERIES_X)\n",
    "    #train_test_split_module\n",
    "    X_train, X_test, y_train, y_test = train_test_split(DECEMBER_SERIES_X, APRIL_SERIES_Y, test_size=number,random_state=50)\n",
    "    model = lm.fit(X_train,y_train)\n",
    "    model.intercept_\n",
    "    model.coef_\n",
    "    #setting the prediciton variable \n",
    "    APRIL_PREDICTION = model.predict(APRIL_SERIES_X)\n",
    "    #50 bins were picked for all of the models below\n",
    "    #Distrubution of Errors pred vs actual\n",
    "    plt.hist(APRIL_PREDICTION-APRIL_SERIES_Y,bins = 50,color =\"black\")\n",
    "    plt.xlabel(\"meters(m)\")\n",
    "    plt.ylabel(\"distribution\")\n",
    "    plt.title(\"Distrbution of predicted minus actual\")\n",
    "    plt.show()\n",
    "    #Displaying Mean Squared Error\n",
    "    twenty_MSE = mean_squared_error(APRIL_SERIES_Y,APRIL_PREDICTION)\n",
    "    print(\"The mean squared error for this model is\",to_precision(twenty_MSE,4),\"%.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Running Diffrent Day Prediction method on dataset \n",
    "different_Day_Prediction(tz3DF,0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "different_Day_Prediction(tz3DF,0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "different_Day_Prediction(tz3DF,0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "different_Day_Prediction(tz3DF,0.4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "different_Day_Prediction(tz3DF,0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "different_Day_Prediction(tz3DF,0.6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "different_Day_Prediction(tz3DF,0.7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "different_Day_Prediction(tz3DF,0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "different_Day_Prediction(tz3DF,0.9)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "248.172px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
